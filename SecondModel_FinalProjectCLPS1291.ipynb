{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmagavi/FinalProject1291/blob/main/SecondModel_FinalProjectCLPS1291.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Project: Classifying Brain Tumors With Convolutional Neural Networks and a Foundational Model**"
      ],
      "metadata": {
        "id": "tzV61X-4nQTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model:** Vision Transformer (ViT) \\\n",
        "**Source:** http://pytorch.org/vision/stable/models/vision_transformer.html"
      ],
      "metadata": {
        "id": "MLsDQR34ndhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import Data**\n",
        "\n",
        "\n",
        "Importing the data from https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset/data\n",
        "\n",
        "**cjdata.label:** 1 for meningioma, 2 for glioma, 3 for pituitary tumor\n",
        "\n",
        "**cjdata.PID:** patient ID\n",
        "\n",
        "**cjdata.image:** image data\n",
        "\n",
        "**cjdata.tumorBorder:** a vector storing the coordinates of discrete points on tumor border.\n",
        "\n",
        "\t\tFor example, [x1, y1, x2, y2,...] in which x1, y1 are planar coordinates on tumor border.\n",
        "\t\tIt was generated by manually delineating the tumor border. So we can use it to generate\n",
        "\t\tbinary image of tumor mask.\n",
        "\n",
        "**cjdata.tumorMask:** a binary image with 1s indicating tumor region\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ruFhtY-VoTGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "q0oZy8KhoS5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing and Downloading Images\n",
        "\n",
        "!gdown 1p_q_wHJZQIEeinSUskvQHtqu88s2gfq8\n",
        "!gdown 1EQRVW5M31GzZjEXnULygxpvBnIGOZ6WA\n",
        "!gdown 1QxVOVJ89AHKQkbW-3Ftu-ajcDuzTTrIZ\n",
        "!gdown 1A1dYyanURV6B7EPgI7VS-Y6tZuheEjJy\n",
        "!unzip brainTumorDataPublic_22993064.zip -d tumor_data\n",
        "!unzip brainTumorDataPublic_15332298.zip -d tumor_data\n",
        "!unzip brainTumorDataPublic_7671532.zip -d tumor_data\n",
        "!unzip brainTumorDataPublic_1766.zip -d tumor_data"
      ],
      "metadata": {
        "id": "2LIUZBsWnSUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!ls tumor_data # Checking Data was loaded correctly into folder."
      ],
      "metadata": {
        "id": "IkIBZyLXsihq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vision Transformer (ViT) Model**\n",
        "Creating, Evaluating and Training a ViT Model on our data\n",
        "\n"
      ],
      "metadata": {
        "id": "pw0k5rwPs8i_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Up**\n",
        "\n",
        "- Downloading packages\n",
        "- Setting up environment"
      ],
      "metadata": {
        "id": "vpkDGu9utad4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env CUDA_LAUNCH_BLOCKING=1 TORCH_USE_CUDA_DSA\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "#SETUP\n",
        "import time\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [7, 7]\n",
        "plt.ion()\n",
        "import cv2\n",
        "import pickle\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchsummary import summary\n",
        "import json\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on\", device)"
      ],
      "metadata": {
        "id": "hH0qj64gteZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instantiating Model**\n",
        "\n",
        "torchvision.models.vit_b_32()"
      ],
      "metadata": {
        "id": "WrGYJqt7trGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vit = models.vit_b_16(weights=\"IMAGENET1K_V1\")\n",
        "vit = vit.to(device)\n",
        "vit.eval()\n",
        "#summary(vit, (3, 224, 224))"
      ],
      "metadata": {
        "id": "r2P5lWgKtuZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bE8T8X5476Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py # Import Python Library to interact with HDF5 files\n",
        "\n",
        "image_path = \"tumor_data/1571.mat\" # using one image to see model works\n",
        "\n",
        "# Open the .mat file using h5py\n",
        "with h5py.File(image_path, 'r') as file:\n",
        "    label = file['cjdata']['label'][()][0, 0]\n",
        "    patient_id = file['cjdata']['PID'][()][0, 0]\n",
        "    image_data = file['cjdata']['image'][()]\n",
        "    tumor_border = file['cjdata']['tumorBorder'][()].flatten() #make it into a 1D array of data on the border"
      ],
      "metadata": {
        "id": "8r-Vu2aI1NTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Similar code to assignment 7\n",
        "# Preprocessing images\n",
        "\n",
        "!gdown 13teI_njgCL6oXbGFVYrDCyhkcOYEhOtj #imagenet_classes.json\n",
        "\n",
        "\n",
        "def preprocess(image_data):\n",
        "    data = np.repeat(image_data[..., np.newaxis], 3, -1)\n",
        "    image = Image.fromarray((data * 255).astype(np.uint8))\n",
        "\n",
        "    # Resize and normalize using PyTorch transforms\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    normalized_image = transform(image)\n",
        "    #print(normalized_image.shape)\n",
        "    return normalized_image\n",
        "\n",
        "\n",
        "PATH_TO_LABELS = 'imagenet_classes.json'\n",
        "with open(PATH_TO_LABELS, 'r') as f:\n",
        "    imagenet_classes = json.load(f)\n",
        "\n",
        "\n",
        "def decode_preds(outputs, class_names=imagenet_classes):\n",
        "  # Assuming outputs is the tensor of model outputs\n",
        "  softmax_outputs = F.softmax(outputs, dim=1)\n",
        "  probability, predicted_class = torch.max(softmax_outputs, dim=1)\n",
        "\n",
        "  predicted_class_labels = [class_names[str(idx)] for idx in predicted_class.cpu().numpy()]\n",
        "  probability_scores = probability.cpu().numpy()\n",
        "\n",
        "  # Print or return the results\n",
        "  for label, score in zip(predicted_class_labels, probability_scores):\n",
        "      print(f'\\nClass: {label}, Probability: {score}')"
      ],
      "metadata": {
        "id": "Ukm_KI566eep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying ViT before finetuning for our images"
      ],
      "metadata": {
        "id": "ckiI6wHOmD59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "array = preprocess(image_data)\n",
        "\n",
        "with torch.no_grad():\n",
        "  vector = vit(array[None, :, :, :].cuda())"
      ],
      "metadata": {
        "id": "ZmlB1VNK-sXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the Image + Probability\n",
        "plt.imshow(image_data)\n",
        "decode_preds(vector)"
      ],
      "metadata": {
        "id": "fpuJ4urN6mpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FineTuning the ViT Model**"
      ],
      "metadata": {
        "id": "e2RL8Ld5m0c9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "\n",
        "# Create a Training, Testing, and Validation set\n",
        "\n",
        "path = \"tumor_data\"\n",
        "file_paths = [os.path.join(path, file) for file in os.listdir(path)]\n",
        "train_files, test_val_files = train_test_split(file_paths, test_size=0.2, random_state=42)\n",
        "test_files, val_files = train_test_split(test_val_files, test_size=0.5, random_state=42)\n",
        "\n",
        "def move_files(file_list, destination_folder):\n",
        "    os.makedirs(destination_folder, exist_ok=True)\n",
        "    for file in file_list:\n",
        "        shutil.move(file, destination_folder)\n",
        "\n",
        "# Move files to their folders\n",
        "# move_files(train_files, \"tumor_data/train\")\n",
        "# move_files(val_files, \"tumor_data/validate\")\n",
        "# move_files(test_files, \"tumor_data/test\")\n"
      ],
      "metadata": {
        "id": "TVhYBweX7QLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomMatDataset(Dataset):\n",
        "    def __init__(self, folder_path, transform=None):\n",
        "        self.folder_path = folder_path\n",
        "        self.file_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.mat')]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_paths[idx]\n",
        "\n",
        "        # Open the .mat file using h5py\n",
        "        with h5py.File(file_path, 'r') as file:\n",
        "            label = file['cjdata']['label'][()][0, 0]\n",
        "            label = label - 1\n",
        "            # patient_id = file['cjdata']['PID'][()][0, 0]\n",
        "            image_data = file['cjdata']['image'][()]\n",
        "            # tumor_border = file['cjdata']['tumorBorder'][()].flatten()\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image_data)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "train_dataset = CustomMatDataset(folder_path='tumor_data/train', transform=preprocess)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "test_dataset = CustomMatDataset(folder_path='tumor_data/test', transform=preprocess)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "validate_dataset = CustomMatDataset(folder_path='tumor_data/validate', transform=preprocess)\n",
        "validate_loader = DataLoader(validate_dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "v-rSunm97Tmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the new labels for the new model"
      ],
      "metadata": {
        "id": "7P2sPugfm4p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#class names\n",
        "class_names = {\n",
        "    0: \"meningioma\",\n",
        "    1: \"glioma\",\n",
        "    2: \"pituitary tumor\",\n",
        "}"
      ],
      "metadata": {
        "id": "pQg5TudR98-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Akash's code\n",
        "standard_vit = models.vit_b_32(weights=\"IMAGENET1K_V1\").to(device)\n",
        "\n",
        "# Freeze the layers of the standard VIT\n",
        "for param in standard_vit.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the model for 3-class classification of Brain images, Remove the fully connected layers (classifier) on top\n",
        "standard_vit.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512 * 7 * 7, 3),  #3 classes of tumours\n",
        "            nn.Softmax(dim=1) # Added\n",
        "        )\n",
        "\n",
        "# Now create a new model with the features of the standard VIT and a new classifier (fully connected layers)\n",
        "class NewVIT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NewVIT, self).__init__()\n",
        "        self.oldvit = standard_vit\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.oldvit(x)\n",
        "        return x\n",
        "\n",
        "new_vit = NewVIT()\n",
        "\n",
        "#print(new_vit)"
      ],
      "metadata": {
        "id": "aj70n3Bn9-_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in new_vit.named_parameters():\n",
        "    print(f\"Parameter: {name}, Data Type: {param.dtype}\")"
      ],
      "metadata": {
        "id": "Stwck-j9KXZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code adapted from: https://medium.com/mlearning-ai/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets.mnist import MNIST\n",
        "\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "def main(model, epochs, lr, train_loader, test_loader, validate_loader):\n",
        "\n",
        "    # Training loop\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "    print(model.parameters())\n",
        "    criterion = CrossEntropyLoss()\n",
        "\n",
        "    for epoch in trange(epochs, desc=\"Training\"):\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
        "            x, y = batch\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_hat = model(x)\n",
        "            y_hat = y_hat.float()\n",
        "\n",
        "            loss = criterion(y_hat, y)\n",
        "\n",
        "\n",
        "            train_loss += loss.detach().cpu().item() / len(train_loader)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} loss: {train_loss:.2f}\")\n",
        "\n",
        "    # Test loop\n",
        "    with torch.no_grad():\n",
        "        correct, total = 0, 0\n",
        "        test_loss = 0.0\n",
        "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "            x, y = batch\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_hat = model(x)\n",
        "            loss = criterion(y_hat, y)\n",
        "            test_loss += loss.detach().cpu().item() / len(test_loader)\n",
        "\n",
        "            correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
        "            total += len(x)\n",
        "        print(f\"Test loss: {test_loss:.2f}\")\n",
        "        print(f\"Test accuracy: {correct / total * 100:.2f}%\")\n",
        "\n",
        "# Assuming new_vit is your modified Vision Transformer model\n",
        "main(new_vit, 20, 0.001, train_loader, test_loader, validate_loader)\n"
      ],
      "metadata": {
        "id": "EyvckBO0-Dp3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}